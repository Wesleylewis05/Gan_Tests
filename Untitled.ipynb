{
 "cells": [
  {
   "cell_type": "raw",
   "id": "cab9ce0e",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Model, load_model, Sequential # for assembling a Neural Network model\n",
    "from keras.layers import ZeroPadding1D, Input, Dense, Embedding, Reshape, Concatenate, Flatten, Dropout, Conv1DTranspose # for adding layers\n",
    "from keras.layers import Multiply, Conv2D, Activation, Conv2DTranspose, MaxPool2D, ReLU, LeakyReLU, Conv1D, Flatten, MaxPooling1D, BatchNormalization, LayerNormalization # for adding layers\n",
    "from tensorflow.keras.utils import plot_model # for plotting model diagram\n",
    "from tensorflow.keras.optimizers import Adam # for model optimization \n",
    "from sklearn import preprocessing\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.constraints import Constraint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from keras.models import load_model\n",
    "import pydot\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import metrics\n",
    "from keras import backend as K\n",
    "# Data manipulation\n",
    "import numpy as np # for data manipulation\n",
    "import pandas as  pd\n",
    "import numpy.matlib\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from numpy import zeros\n",
    "# Visualization\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt # for data visualization\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple\n",
    "from keras import initializers\n",
    "from keras.callbacks import Callback\n",
    "from keras.initializers import RandomNormal\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "np.random.seed(1600)\n",
    "tf.random.set_seed(1600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f6c2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in cpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bffbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in all data\n",
    "read_files = os.listdir('data/container_content')\n",
    "readings = {z.split('.')[0]:pd.read_csv(os.path.join('./data/container_content',z)) for z in read_files}\n",
    "# Remove extra data fields from the readings\n",
    "for key in readings.keys():\n",
    "    try:\n",
    "        readings[key] = readings[key].loc[:,list(readings[key].columns)[:-4]].to_numpy().astype(np.int32)\n",
    "    except Exception as e:\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89a1332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in calibration data\n",
    "hamamatsu_dark = np.median(pd.read_csv('./calibration/hamamatsu_black_ref.csv').to_numpy().astype(np.int32), axis=0)\n",
    "hamamatsu_white = np.median(pd.read_csv('./calibration/hamamatsu_white_ref.csv').to_numpy().astype(np.int32), axis=0)\n",
    "mantispectra_dark = np.median(pd.read_csv('./calibration/mantispectra_black_ref.csv').to_numpy()[:,:-5].astype(np.int32), axis=0)\n",
    "mantispectra_white = np.median(pd.read_csv('./calibration/mantispectra_white_ref.csv').to_numpy()[:,:-5].astype(np.int32), axis=0)\n",
    "\n",
    "# Create composite calibration file\n",
    "white_ref = np.concatenate((hamamatsu_white, mantispectra_white))[1:]\n",
    "dark_ref = np.concatenate((hamamatsu_dark, mantispectra_dark))[1:]\n",
    "\n",
    "# Create calibration function\n",
    "def spectral_calibration(reading):\n",
    "    t = np.divide((reading-dark_ref), (white_ref-dark_ref), where=(white_ref-dark_ref)!=0)\n",
    "    # Handle cases where there is null division, which casts values as \"None\"\n",
    "    if np.sum(t==None) > 0:\n",
    "        print('Null readings!')\n",
    "    t[t== None] = 0\n",
    "    # Handle edge cases with large spikes in data, clip to be within a factor of the white reference to avoid skewing the model\n",
    "    t = np.clip(t,-2.5,2.5)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393892c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrate all the data\n",
    "readings_cal = {}\n",
    "for key in readings.keys():\n",
    "    readings_cal[key] = np.apply_along_axis(spectral_calibration,1,readings[key])\n",
    "\n",
    "readings_cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3edd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the container-substrate pairings\n",
    "pairings = pd.read_csv('./data/container_substrate.csv',header=1, keep_default_na=False)\n",
    "# Remove blank data rows\n",
    "pairings = pairings.loc[:18,(pairings.columns)[:20]]\n",
    "# Unique substances\n",
    "contents = list(pairings.columns[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b27ab46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Containers to exclude - wood, stainless steel, aluminum\n",
    "exclude_containers = ['O','P','Q','I','K','M']\n",
    "exclude_contents = [15,2,0,7,10]\n",
    "# Generalized function to group data by the contents type\n",
    "\n",
    "def random_scale(reading: np.array) -> np.array:\n",
    "    return reading * np.random.default_rng().normal(1.0,0.05,1)[0]\n",
    "\n",
    "def generate_data_labels(readings: Dict) -> defaultdict:\n",
    "    data_by_contents = np.array([])\n",
    "    labels_by_contents = np.array([])\n",
    "\n",
    "    # Iterate over all data_frames types\n",
    "    for key in readings.keys():\n",
    "        # Iterate over all containers, but skip Aluminum (P), Stainless Steel (Q), and Wood (R)\n",
    "        if key[0] in exclude_containers or (len(key) > 1 and int(key[1:]) in exclude_contents): #:or int(key[1:]) in exclude_contents:\n",
    "            continue\n",
    "        for index, val in enumerate(contents):\n",
    "            if key not in list(pairings[val]):\n",
    "                continue\n",
    "            # Otherwise the data is useful to use, let's proceed with the data wrangling\n",
    "            useData = readings[key]\n",
    "            # ADD SCALING NOISE TO THE DATA HERE\n",
    "            useData = np.matlib.repmat(useData,3,1)\n",
    "            useData = np.apply_along_axis(random_scale,1,useData)\n",
    "            # Get the plain name of the container\n",
    "            useContainer = pairings[np.equal.outer(pairings.to_numpy(copy=True),  [key]).any(axis=1).all(axis=1)]['container / substrate'].iloc[0]\n",
    "            # Add the index as the key value\n",
    "            data_by_contents = np.vstack((data_by_contents, useData)) if data_by_contents.size else useData\n",
    "            labels_by_contents = np.vstack((labels_by_contents, np.matlib.repmat([val,useContainer],useData.shape[0],1))) if labels_by_contents.size else np.matlib.repmat([val,useContainer],useData.shape[0],1)\n",
    "    return data_by_contents, labels_by_contents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5661807",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data, labels = generate_data_labels(readings_cal)\n",
    "all_data = np.hstack((all_data,np.gradient(all_data,axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acb0314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit labels to model\n",
    "le_contents = preprocessing.LabelEncoder()\n",
    "le_containers = preprocessing.LabelEncoder()\n",
    "labels_contents = le_contents.fit_transform(labels[:,0])\n",
    "labels_containers = le_containers.fit_transform(labels[:,1])\n",
    "encoded_labels = np.vstack((labels_containers,labels_contents)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a0bf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(all_data)\n",
    "data[\"labels\"] = labels_contents\n",
    "classes = len(np.unique(labels_contents))\n",
    "classesx = (np.unique(labels_contents))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dbfd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbe06e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop([\"labels\"], axis=1)\n",
    "y = data[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d89884",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, stratify=y_train)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305e9b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata_x = X_train.copy()\n",
    "newdata_y = y_train.copy()\n",
    "newdata = pd.DataFrame(newdata_x)\n",
    "newdata[\"labels\"] = (newdata_y)\n",
    "newdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0117a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_ = data.shape[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de98c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_classes = classes\n",
    "latent_dim = 100\n",
    "     \n",
    "\n",
    "class cWGAN(Model):\n",
    "    def __init__(self, num_classes, d_steps=5, img_rows=X_train.shape[1], latent_dim=100):\n",
    "        super(cWGAN, self).__init__()\n",
    "\n",
    "        self.img_rows = img_rows\n",
    "        self.channels = 3\n",
    "        self.img_shape = (self.img_rows,)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.d_steps = d_steps\n",
    "        self.gp_weight = 10.0\n",
    "\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        plot_model(self.discriminator,show_shapes=True)\n",
    "\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "\n",
    "    def compile(self):\n",
    "        super(cWGAN, self).compile()\n",
    "        self.d_optimizer = Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.9)\n",
    "        self.g_optimizer = Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.9)\n",
    "\n",
    "\n",
    "    def discriminator_loss(self, real_img, fake_img):\n",
    "        real_loss = tf.reduce_mean(real_img)\n",
    "        fake_loss = tf.reduce_mean(fake_img)\n",
    "        return fake_loss - real_loss\n",
    "\n",
    "    # the loss functions for the generator (note that its negative)\n",
    "    def generator_loss(self, fake_img):\n",
    "        return -tf.reduce_mean(fake_img)\n",
    "\n",
    "\n",
    "    def gradient_penalty(self, batch_size, real_images, fake_images, labels):\n",
    "        \"\"\"Calculates the gradient penalty.\n",
    "        This loss is calculated on an interpolated image\n",
    "        and added to the discriminator loss.\n",
    "        \"\"\"\n",
    "        # Get the interpolated image\n",
    "        alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "        diff = fake_images - real_images\n",
    "        interpolated = real_images + alpha * diff\n",
    "\n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated)\n",
    "            # 1. Get the discriminator output for this interpolated image.\n",
    "            pred = self.discriminator([interpolated, labels], training=True)\n",
    "\n",
    "        # 2. Calculate the gradients w.r.t to this interpolated image.\n",
    "        grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "        # 3. Calculate the norm of the gradients.\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
    "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "        return gp\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        init = RandomNormal(stddev=0.02)\n",
    "\n",
    "        model = Sequential(name=\"generator\")\n",
    "\n",
    "        model.add(Dense(self.img_rows, input_dim=self.latent_dim, kernel_initializer=init))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Dense(256, kernel_initializer=init))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Dense(128, kernel_initializer=init))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Dense(64, kernel_initializer=init))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Dense(self.img_rows, kernel_initializer=init, name=\"bob\"))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "\n",
    "        label = Input(shape=(1,))\n",
    "\n",
    "        label_embedding = Embedding(self.num_classes, self.latent_dim, input_length = 1)(label)\n",
    "        label_embedding = Flatten()(label_embedding)\n",
    "        joined = Multiply()([noise, label_embedding])\n",
    "\n",
    "        img = model(joined)\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        return Model([noise, label], img)\n",
    "\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        init = RandomNormal(stddev=0.02)\n",
    "        model = Sequential(name=\"discriminator\")\n",
    "\n",
    "        model.add(Dense(64, input_shape=(self.img_rows,), kernel_initializer=init))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Dense(128, kernel_initializer=init)) # downsample to 40x40\n",
    "        model.add(LayerNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Dense(256, kernel_initializer=init)) # downsample to 20x20\n",
    "        model.add(LayerNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Dense(512, kernel_initializer=init)) # downsample to 10x10\n",
    "        model.add(LayerNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1))\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "\n",
    "\n",
    "        label = Input(shape= (1,))\n",
    "\n",
    "        label_embedding = Embedding(input_dim = self.num_classes, output_dim = np.prod(self.img_shape), input_length = 1)(label)\n",
    "        label_embedding = Flatten()(label_embedding)\n",
    "\n",
    "        concat = Concatenate(axis = 0)([img, label_embedding])\n",
    "        prediction = model(concat)\n",
    "\n",
    "        model.summary()\n",
    "        return Model([img, label], prediction)\n",
    "\n",
    "\n",
    "    def train_step(self, data):\n",
    "\n",
    "        real_images, labels = data\n",
    "\n",
    "        if isinstance(real_images, tuple):\n",
    "            real_images = real_images[0]\n",
    "\n",
    "\n",
    "        # the batch size\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "\n",
    "\n",
    "        # train discriminator\n",
    "        for i in range(self.d_steps):\n",
    "\n",
    "            random_latent_vectors = tf.random.normal \\\n",
    "                (shape=(batch_size, self.latent_dim)) # get points from latent vector\n",
    "\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "\n",
    "                fake_images = self.generator([random_latent_vectors, labels], training=True) # decode to fake images\n",
    "\n",
    "                fake_logits = self.discriminator([fake_images, labels], training=True) # fake images\n",
    "                real_logits = self.discriminator([real_images, labels], training=True) # real images\n",
    "\n",
    "\n",
    "                d_cost = self.discriminator_loss(real_img=real_logits, fake_img=fake_logits) # using the fake and real image logits get discriminator loss\n",
    "\n",
    "                # gradient penalty\n",
    "                gp = self.gradient_penalty(batch_size, real_images, fake_images, labels)\n",
    "                d_loss = d_cost + gp * self.gp_weight # add gradient penalty to original loss\n",
    "\n",
    "            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "            self.d_optimizer.apply_gradients(zip(d_gradient, self.discriminator.trainable_variables))\n",
    "\n",
    "\n",
    "        # train the generator\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            generated_images = self.generator([random_latent_vectors, labels], training=True) # get fake imgs\n",
    "\n",
    "            gen_img_logits = self.discriminator([generated_images, labels], training=True) # discriminator logits for fake images\n",
    "            g_loss = self.generator_loss(gen_img_logits)\n",
    "\n",
    "        # get the gradients with respect to the generator loss\n",
    "        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        self.g_optimizer.apply_gradients(zip(gen_gradient, self.generator.trainable_variables))\n",
    "\n",
    "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}\n",
    "\n",
    "\n",
    "class GANMonitor(Callback):\n",
    "    def __init__(self, epoch_summarize=5, n=7, latent_dim=100):\n",
    "        self.epoch_summarize = epoch_summarize\n",
    "        self.n = n\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "\n",
    "        if (epoch +1) % self.epoch_summarize == 0:\n",
    "\n",
    "            random_latent_vectors = tf.random.normal(shape=(self.n * self.model.num_classes, self.latent_dim))\n",
    "            labels = tf.repeat(tf.reshape(tf.range(0, self.model.num_classes), (-1 ,1)), self.n)\n",
    "            generated_images = self.model.generator([random_latent_vectors, labels])  # ????\n",
    "            generated_images = (generated_images + 1) / 2.0 # scale from [-1,1] to [0,1]\n",
    "\n",
    "            for i in range(self.n * self.model.num_classes):\n",
    "                plt.subplot(self.n, self.model.num_classes, 1 + i)\n",
    "                plt.axis('off')\n",
    "                plt.imshow(generated_images[i])\n",
    "\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf1e524",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbk = GANMonitor(epoch_summarize=10, latent_dim=latent_dim)\n",
    "cwgan = cWGAN(num_classes=num_classes, d_steps=5, latent_dim=latent_dim)\n",
    "cwgan.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf26104",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.data.experimental.enable_debug_mode()\n",
    "cwgan.fit(X_train, y_train, batch_size=256, epochs=200, callbacks=[cbk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5489ae75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bb6248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8845f583",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
