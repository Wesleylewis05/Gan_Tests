{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fef183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Model, load_model, Sequential # for assembling a Neural Network model\n",
    "from keras.layers import ZeroPadding1D, Input, Dense, Embedding, Reshape, Concatenate, Flatten, Dropout, Conv1DTranspose # for adding layers\n",
    "from keras.layers import Conv2D, Conv2DTranspose, MaxPool2D, ReLU, LeakyReLU, Conv1D, Flatten, MaxPooling1D, BatchNormalization # for adding layers\n",
    "from tensorflow.keras.utils import plot_model # for plotting model diagram\n",
    "from tensorflow.keras.optimizers import Adam # for model optimization \n",
    "import numpy as np # for data manipulation\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.constraints import Constraint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from keras.models import load_model\n",
    "import pydot\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn import metrics\n",
    "from keras import backend as K\n",
    "# Data manipulation\n",
    "import pandas as  pd\n",
    "import numpy.matlib\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from numpy import zeros\n",
    "# Visualization\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt # for data visualization\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple\n",
    "from keras import initializers\n",
    "np.random.seed(1600)\n",
    "tf.random.set_seed(1600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3740393c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in cpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6660b47",
   "metadata": {},
   "source": [
    "# Load the Spectroscopy DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95080593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in all data\n",
    "read_files = os.listdir('data/container_content')\n",
    "readings = {z.split('.')[0]:pd.read_csv(os.path.join('./data/container_content',z)) for z in read_files}\n",
    "# Remove extra data fields from the readings\n",
    "for key in readings.keys():\n",
    "    try:\n",
    "        readings[key] = readings[key].loc[:,list(readings[key].columns)[:-4]].to_numpy().astype(np.int32)\n",
    "    except Exception as e:\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3f31ab",
   "metadata": {},
   "source": [
    "### Perform reflectance calibraiton across all data samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e72859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in calibration data\n",
    "hamamatsu_dark = np.median(pd.read_csv('./calibration/hamamatsu_black_ref.csv').to_numpy().astype(np.int32), axis=0)\n",
    "hamamatsu_white = np.median(pd.read_csv('./calibration/hamamatsu_white_ref.csv').to_numpy().astype(np.int32), axis=0)\n",
    "mantispectra_dark = np.median(pd.read_csv('./calibration/mantispectra_black_ref.csv').to_numpy()[:,:-5].astype(np.int32), axis=0)\n",
    "mantispectra_white = np.median(pd.read_csv('./calibration/mantispectra_white_ref.csv').to_numpy()[:,:-5].astype(np.int32), axis=0)\n",
    "\n",
    "# Create composite calibration file\n",
    "white_ref = np.concatenate((hamamatsu_white, mantispectra_white))[1:]\n",
    "dark_ref = np.concatenate((hamamatsu_dark, mantispectra_dark))[1:]\n",
    "\n",
    "# Create calibration function\n",
    "def spectral_calibration(reading):\n",
    "    t = np.divide((reading-dark_ref), (white_ref-dark_ref), where=(white_ref-dark_ref)!=0)\n",
    "    # Handle cases where there is null division, which casts values as \"None\"\n",
    "    if np.sum(t==None) > 0:\n",
    "        print('Null readings!')\n",
    "    t[t== None] = 0\n",
    "    # Handle edge cases with large spikes in data, clip to be within a factor of the white reference to avoid skewing the model\n",
    "    t = np.clip(t,-2.5,2.5)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d057cb",
   "metadata": {},
   "source": [
    "### Reflectance normalize spectral readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1d63f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrate all the data\n",
    "readings_cal = {}\n",
    "for key in readings.keys():\n",
    "    readings_cal[key] = np.apply_along_axis(spectral_calibration,1,readings[key])\n",
    "\n",
    "readings_cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68b9296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the container-substrate pairings\n",
    "pairings = pd.read_csv('./data/container_substrate.csv',header=1, keep_default_na=False)\n",
    "# Remove blank data rows\n",
    "pairings = pairings.loc[:18,(pairings.columns)[:20]]\n",
    "# Unique substances\n",
    "contents = list(pairings.columns[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac7b237",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928796a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adedecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Containers to exclude - wood, stainless steel, aluminum\n",
    "exclude_containers = ['O','P','Q','I','K','M']\n",
    "exclude_contents = [15,2,0,7,10]\n",
    "# Generalized function to group data by the contents type\n",
    "\n",
    "def random_scale(reading: np.array) -> np.array:\n",
    "    return reading * np.random.default_rng().normal(1.0,0.05,1)[0]\n",
    "\n",
    "def generate_data_labels(readings: Dict) -> defaultdict:\n",
    "    data_by_contents = np.array([])\n",
    "    labels_by_contents = np.array([])\n",
    "\n",
    "    # Iterate over all data_frames types\n",
    "    for key in readings.keys():\n",
    "        # Iterate over all containers, but skip Aluminum (P), Stainless Steel (Q), and Wood (R)\n",
    "        if key[0] in exclude_containers or (len(key) > 1 and int(key[1:]) in exclude_contents): #:or int(key[1:]) in exclude_contents:\n",
    "            continue\n",
    "        for index, val in enumerate(contents):\n",
    "            if key not in list(pairings[val]):\n",
    "                continue\n",
    "            # Otherwise the data is useful to use, let's proceed with the data wrangling\n",
    "            useData = readings[key]\n",
    "            # ADD SCALING NOISE TO THE DATA HERE\n",
    "            useData = np.matlib.repmat(useData,3,1)\n",
    "            useData = np.apply_along_axis(random_scale,1,useData)\n",
    "            # Get the plain name of the container\n",
    "            useContainer = pairings[np.equal.outer(pairings.to_numpy(copy=True),  [key]).any(axis=1).all(axis=1)]['container / substrate'].iloc[0]\n",
    "            # Add the index as the key value\n",
    "            data_by_contents = np.vstack((data_by_contents, useData)) if data_by_contents.size else useData\n",
    "            labels_by_contents = np.vstack((labels_by_contents, np.matlib.repmat([val,useContainer],useData.shape[0],1))) if labels_by_contents.size else np.matlib.repmat([val,useContainer],useData.shape[0],1)\n",
    "    return data_by_contents, labels_by_contents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba3df48",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data, labels = generate_data_labels(readings_cal)\n",
    "all_data = np.hstack((all_data,np.gradient(all_data,axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f55882",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d71bdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(labels[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26c69b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit labels to model\n",
    "le_contents = preprocessing.LabelEncoder()\n",
    "le_containers = preprocessing.LabelEncoder()\n",
    "labels_contents = le_contents.fit_transform(labels[:,0])\n",
    "labels_containers = le_containers.fit_transform(labels[:,1])\n",
    "encoded_labels = np.vstack((labels_containers,labels_contents)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8276ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels, labels_contents, labels_containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40d0573",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(all_data)\n",
    "data[\"labels\"] = labels_contents\n",
    "classes = len(np.unique(labels_contents))\n",
    "classesx = (np.unique(labels_contents))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9ae577",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop([\"labels\"], axis=1)\n",
    "y = data[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2704128d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, stratify=y_train)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a19744f",
   "metadata": {},
   "source": [
    "## Put scaled data in separate dataframe for future visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48426106",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata_x = X_train.copy()\n",
    "newdata_y = y_train.copy()\n",
    "newdata = pd.DataFrame(newdata_x)\n",
    "newdata[\"labels\"] = (newdata_y)\n",
    "newdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aa610d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fba3c22",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716a6499",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_ = data.shape[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d04068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(200, activation=\"relu\", input_shape=(X_train.shape[1],)))\n",
    "    model.add(Dense(100, activation=\"relu\"))\n",
    "    model.add(Dense(25, activation=\"relu\"))\n",
    "    \n",
    "    model.add(Dense(classes, activation = 'softmax'))\n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer=\"SGD\",metrics = ['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "mlp_model = MLP_()\n",
    "mlp_model.summary()\n",
    "plot_model(mlp_model,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73224812",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model.fit(X_train, y_train, batch_size=64,epochs=500, verbose=1, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa51f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = mlp_model.evaluate(X_test, y_test)\n",
    "print(\"Loss:\", acc[0], \" Accuracy:\", acc[1])\n",
    "pred = mlp_model.predict(X_test)\n",
    "pred_y = pred.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a09966a",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(y_test, pred_y)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels =classesx)\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071b8764",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Actual: {y_test}\")\n",
    "print(f\"Predicted: {pred_y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f7479e",
   "metadata": {},
   "source": [
    "### Begin WCGAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c0b933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latent_points(latent_dim, n_samples, n_classes=classes):\n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    z_input = x_input.reshape(n_samples, latent_dim)\n",
    "    # generate labels\n",
    "    labels = randint(0, n_classes, n_samples)\n",
    "    return [z_input, labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6839ee71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "    # generate points in latent space\n",
    "    z_input, labels_input = generate_latent_points(latent_dim, n_samples)\n",
    "    # predict outputs\n",
    "    images = generator.predict([z_input, labels_input])\n",
    "    # create class labels\n",
    "    y = -np.ones((n_samples, 1))\n",
    "    return [images, labels_input], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c937e1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate n real samples with class labels; We randomly select n samples from the real data\n",
    "def generate_real_samples(n):\n",
    "    X = data.sample(n)\n",
    "    x = X.iloc[:,0:606]\n",
    "    labels = X.iloc[:,-1]\n",
    "    y = np.ones((n, 1))    \n",
    "    return [x, labels], y\n",
    "generate_real_samples(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d585a1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_generator(latent_dim, c=classes):\n",
    "    # label input\n",
    "    in_label = Input(shape=(1,))\n",
    "    n_nodes = 1 * X_train.shape[1]\n",
    "\n",
    "    # image generator input\n",
    "    n_nodes = 1 * X_train.shape[1]\n",
    "    # Noise input\n",
    "    in_lat = Input(shape=(latent_dim,))\n",
    "    gen = Dense(n_nodes)(in_lat)\n",
    "    gen = LeakyReLU(alpha=0.2)(gen)\n",
    "    # merge image gen and label input\n",
    "    merge = Concatenate()([gen, in_label])\n",
    "    \n",
    "    gen = Dense(128)(merge)\n",
    "    gen = LeakyReLU(alpha=0.2)(gen)\n",
    "    \n",
    "    gen = Dense(256)(gen)\n",
    "    gen = LeakyReLU(alpha=0.2)(gen)\n",
    "    \n",
    "    gen = Dense(512)(gen)\n",
    "    gen = LeakyReLU(alpha=0.2)(gen)\n",
    "    \n",
    "    # output\n",
    "    out_layer = Dense(X_train.shape[1], activation=\"tanh\")(gen)\n",
    "    model = Model([in_lat, in_label], out_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c40343",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator1 = define_generator(100)\n",
    "generator1.summary()\n",
    "plot_model(generator1,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93883a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of wasserstein loss\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "     return K.mean((y_true+1e-5) * (y_pred+1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4fa500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty_loss(y_true, y_pred, averaged_samples):\n",
    "        # Computes gradient penalty based on prediction and weighted real / fake samples\n",
    "        gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "        # compute the euclidean norm by squaring ...\n",
    "        gradients_sqr = K.square(gradients)\n",
    "        #   ... summing over the rows ...\n",
    "        gradients_sqr_sum = K.sum(gradients_sqr, axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "        #   ... and sqrt\n",
    "        gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "        # compute lambda * (1 - ||grad||)^2 still for each single sample\n",
    "        gradient_penalty = K.square(1 - gradient_l2_norm)\n",
    "        # return the mean as loss over all the batch samples\n",
    "        return K.mean(gradient_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a3c64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake):\n",
    "        return -tf.reduce_mean(fake)\n",
    "    \n",
    "def discriminator_loss(real, fake):\n",
    "        real_loss = tf.reduce_mean(real)\n",
    "        fake_loss = tf.reduce_mean(fake)\n",
    "        return fake_loss - real_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf4b3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip model weights to a given hypercube\n",
    "class ClipConstraint(Constraint):\n",
    "    # set clip value when initialized\n",
    "    def __init__(self, clip_value):\n",
    "        self.clip_value = clip_value\n",
    "\n",
    "    # clip model weights to hypercube\n",
    "    def __call__(self, weights):\n",
    "        return K.clip(weights, -self.clip_value, self.clip_value)\n",
    "\n",
    "    # get the config\n",
    "    def get_config(self):\n",
    "        return {'clip_value': self.clip_value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cf356e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a8d33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_discriminator(n_inputs=X_train.shape[1], n_c = classes):\n",
    "    # weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "     # weight constraint\n",
    "    const = ClipConstraint(0.01)\n",
    "    in_shape = (X_train.shape[1],)\n",
    "    # label input\n",
    "    in_label = Input((1,))\n",
    "    n_nodes = X_train.shape[1] * 1\n",
    "    # image input\n",
    "    in_image = Input(shape=in_shape)\n",
    "    # concat label as a channel\n",
    "    merge = Concatenate()([in_image, in_label])\n",
    "    \n",
    "    fe = Dense(32)(merge)\n",
    "    fe = LeakyReLU(alpha=0.2)(fe)\n",
    "    \n",
    "    fe = Dense(64)(fe) \n",
    "    fe = LeakyReLU(alpha=0.2)(fe)\n",
    "\n",
    "    out_layer = Dense(1, activation='sigmoid')(fe)\n",
    "    # define model\n",
    "    model = Model([in_image, in_label], out_layer)\n",
    "    opt = RMSprop(learning_rate=0.00005)\n",
    "    model.compile(loss=wasserstein_loss, optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n",
    "discriminator1 = define_discriminator()\n",
    "discriminator1.summary()\n",
    "plot_model(discriminator1,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e1cfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(generator, discriminator):\n",
    "    # make weights in the discriminator not trainable\n",
    "    discriminator.trainable = False    \n",
    "    # get noise and label inputs from generator model\n",
    "    gen_noise, gen_label = generator.input\n",
    "    # get image output from the generator model\n",
    "    gen_output = generator.output\n",
    "    # connect image output and label input from generator as inputs to discriminator\n",
    "    gan_output = discriminator([gen_output, gen_label])\n",
    "    # define gan model as taking noise and label and outputting a classification\n",
    "    model = Model([gen_noise, gen_label], gan_output)\n",
    "    # compile model\n",
    "    opt = RMSprop(learning_rate=0.00005)\n",
    "    model.compile(loss=wasserstein_loss, optimizer=opt) \n",
    "    return model\n",
    "gg = define_gan(generator1,discriminator1)\n",
    "plot_model(gg,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d0ac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a line plot of loss for the gan and save to file\n",
    "def plot_history(d_hist, d2_hist):\n",
    "    # plot loss\n",
    "    plt.plot(d_hist, label='crit_real')\n",
    "    plt.plot(d2_hist, label='crit_fake')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "d_optimizer = RMSprop(learning_rate=0.00005)\n",
    "g_optimizer = RMSprop(learning_rate=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0817a1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the generator and discriminator\n",
    "def train(g_model, d_model, gan_model, latent_dim, n_epochs=30, n_batch=32, dataset=data):    # determine half the size of one batch, for updating the  discriminator\n",
    "    half_batch = int(n_batch / 2)\n",
    "    bat_per_epo = int(dataset[0].shape[0] / n_batch)\n",
    "    d_history = []\n",
    "    g_history = []    # manually enumerate epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        # enumerate batches over the training set\n",
    "        with tf.GradientTape() as tape:\n",
    "            # prepare fake examples\n",
    "            [X_real, labels_real], y_real = generate_real_samples(half_batch)   \n",
    "            # generate 'fake' examples\n",
    "            [X_fake, labels], y_fake = generate_fake_samples(g_model, latent_dim, half_batch)    \n",
    "            # update discriminator\n",
    "            d_loss_real, d_real_acc = d_model.train_on_batch([X_real, labels_real], y_real)\n",
    "            d_loss_fake, d_fake_acc = d_model.train_on_batch([X_fake, labels], y_fake)\n",
    "            # using the fake and realloss get discriminator loss\n",
    "            d_cost = discriminator_loss(d_loss_real, d_loss_fake)\n",
    "            \n",
    "            # gradient penalty\n",
    "            gp = gradient_penalty(half_batch, X_real, X_fake, labels)\n",
    "            # add gradient penalty to original loss\n",
    "            d_loss = d_cost + gp * 10 \n",
    "        d_gradient = tape.gradient(d_loss, d_model.trainable_variables)\n",
    "        d_optimizer.apply_gradients(zip(d_gradient, d_model.trainable_variables))\n",
    "        # prepare points in latent space as input for the generator\n",
    "        [z_input, labels_input] = generate_latent_points(latent_dim, n_batch) \n",
    "        # create inverted labels for the fake samples\n",
    "        y_gan = np.ones((n_batch, 1))    \n",
    "        # update the generator via the discriminator's error\n",
    "        g_loss_fake = gan_model.train_on_batch([z_input, labels_input], y_gan)   \n",
    "        print('>%d, d1=%.3f, d2=%.3f d=%.3f g=%.3f' % (epoch+1, d_loss_real, d_loss_fake, d_loss,  g_loss_fake))    \n",
    "        d_history.append(d_loss)\n",
    "        g_history.append(g_loss_fake)    \n",
    "        plot_history(d_history, g_history)    \n",
    "        g_model.save('trained_generated_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af10de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of the latent space\n",
    "latent_dim = 100 # create the discriminator\n",
    "discriminator = define_discriminator()# create the generator\n",
    "generator = define_generator(latent_dim)# create the gan\n",
    "gan_model = define_gan(generator, discriminator)# train model\n",
    "train(generator, discriminator, gan_model, latent_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e79d7a",
   "metadata": {},
   "source": [
    "### Orange is the loss of the Generator and Blue is the loss of the Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c9e926",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d00e33",
   "metadata": {},
   "source": [
    "### Generate fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267d3607",
   "metadata": {},
   "outputs": [],
   "source": [
    "model =load_model('trained_generated_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f456b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_points, labels = generate_latent_points(100, 16900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc05e265",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.asarray([x for _ in range(classes*100) for x in range(classes)])\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5580e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data and labels\n",
    "Q  = model.predict([latent_points, labels])\n",
    "print(np.shape(Q))\n",
    "Q = (Q + 1) / 2.0\n",
    "Q = np.squeeze(Q, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483dd5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdata = pd.DataFrame(Q)\n",
    "fdata[\"labels\"] = labels\n",
    "fdata = fdata.sample(frac = 1)\n",
    "fdata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873f380b",
   "metadata": {},
   "source": [
    "### Train test split for the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ba83d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "xX_train, xX_test, xy_train, xy_test = train_test_split(data.iloc[:,0:606], data.iloc[:,-1], test_size=0.10, stratify=data.iloc[:,-1])\n",
    "xX_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff5773c",
   "metadata": {},
   "source": [
    "### Append the fake data to X_train and y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab5b4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "xframes = [xX_train, fdata.iloc[:,0:606]]\n",
    "yframes = [xy_train, fdata.iloc[:,-1]]\n",
    "xresult = pd.concat(xframes)\n",
    "yresult = pd.concat(yframes)\n",
    "xresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b694f4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "yresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e027d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(200, activation=\"relu\", input_shape=(X_train.shape[1],)))\n",
    "    model.add(Dense(100, activation=\"relu\"))\n",
    "    model.add(Dense(25, activation=\"relu\"))\n",
    "    \n",
    "    model.add(Dense(classes, activation = 'softmax'))\n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = \"adam\",metrics = ['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "mlp_model = MLP_()\n",
    "mlp_model.summary()\n",
    "plot_model(mlp_model,show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e72326",
   "metadata": {},
   "source": [
    "### Train the model using the fake and real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605b1008",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model.fit(xresult, yresult, batch_size=64,epochs=100, verbose=1, validation_split=0.10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08270dca",
   "metadata": {},
   "source": [
    "### Evaluate the model using the X_test and Y_test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a02b8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = mlp_model.evaluate(xX_test, xy_test)\n",
    "print(\"Loss:\", acc[0], \" Accuracy:\", acc[1])\n",
    "pred = mlp_model.predict(xX_test)\n",
    "pred_y = pred.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368764d5",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0c5438",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(xy_test, pred_y)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels =[0,1,2,3,4,5,6,7,8,9,10,11,12])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9720133",
   "metadata": {},
   "outputs": [],
   "source": [
    "falseplot0 = fdata[fdata['labels']==0].head(50)\n",
    "falseplot10 = fdata[fdata['labels']==10].head(50)\n",
    "\n",
    "falseplot0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a9389d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trueplot0 = newdata[newdata['labels']==0].head(50)\n",
    "trueplot10 = newdata[newdata['labels']==10].head(50)\n",
    "\n",
    "trueplot0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1003ebca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fake_():\n",
    "    temp = 0\n",
    "    for i in range(1,50):\n",
    "        bob  = falseplot10.iloc[temp:i,0:606].values.tolist()\n",
    "        temp += 1\n",
    "        ypoints = bob[0]\n",
    "        plt.plot(ypoints, color=\"b\")\n",
    "    plt.figure(figsize=(20,12))\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def plot_both_():\n",
    "    temp = 0\n",
    "    for i in range(1,50):\n",
    "        bob = trueplot10.iloc[temp:i,0:606].values.tolist()\n",
    "        tom = falseplot10.iloc[temp:i,0:606].values.tolist()\n",
    "        temp += 1\n",
    "        ypoints_real = bob[0]\n",
    "        ypoints_fake = tom[0]\n",
    "        plt.plot(ypoints_real, color=\"r\")\n",
    "        plt.plot(ypoints_fake, color=\"b\")\n",
    "    plt.figure(figsize=(40,24))\n",
    "    plt.show()\n",
    "    \n",
    "def plot_real_():\n",
    "    temp = 0\n",
    "    for i in range(1,50):\n",
    "        bob  = trueplot10.iloc[temp:i,0:606].values.tolist()\n",
    "        temp += 1\n",
    "        ypoints = bob[0]\n",
    "        plt.plot(ypoints, color=\"r\")\n",
    "    plt.figure(figsize=(20,12))\n",
    "\n",
    "    plt.show() \n",
    "plot_real_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd793d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fake_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62109cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_both_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b925a1b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
